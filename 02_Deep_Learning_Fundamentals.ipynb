{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4c87fe-57e2-4f48-a046-9ec4508d34b8",
   "metadata": {},
   "source": [
    "## Lesson Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "* Understand the fundamentals of the __perceptron__ algorithm and basic __neural network__ architecture\n",
    "* Apply __PyTorch__ for practical deep learning tasks\n",
    "* Apply __Hugging Face__ for practical deep learning tasks\n",
    "* Use __transfer learning__ to leverage pre-trained models for a variety of machine learning tasks\n",
    "\n",
    "\n",
    "__Technical Terms Explained:__\n",
    "\n",
    "`Perceptron:` A basic computational model in machine learning that makes decisions by weighing input data. It's like a mini-decision maker that labels data as one thing or another.\n",
    "\n",
    "`Binary Classifier:` A type of system that categorizes data into one of two groups. Picture a light switch that can be flipped to either on or off.\n",
    "\n",
    "`Vector of Numbers:` A sequence of numbers arranged in order, which together represent one piece of data.\n",
    "\n",
    "`Activation Function:` A mathematical equation that decides whether the perceptron's calculated sum from the inputs is enough to trigger a positive or negative output.\n",
    "\n",
    "`Multi-Layer Perceptron (MLP):` A type of artificial neural network that has multiple layers of nodes, each layer learning to recognize increasingly complex features of the input data.\n",
    "\n",
    "`Input Layer:` The first layer in an MLP where the raw data is initially received.\n",
    "\n",
    "`Output Layer:` The last layer in an MLP that produces the final result or prediction of the network.\n",
    "\n",
    "`Hidden Layers:` Layers between the input and output that perform complex data transformations.\n",
    "\n",
    "`Labeled Dataset:` This is a collection of data where each piece of information comes with a correct answer or label. It's like a quiz with the questions and answers already provided.\n",
    "\n",
    "`Gradient Descent:` This method helps find the best settings for a neural network by slowly tweaking them to reduce errors, similar to finding the lowest point in a valley.\n",
    "\n",
    "`Cost Function:` Imagine it as a score that tells you how wrong your network's predictions are. The goal is to make this score as low as possible.\n",
    "\n",
    "`Learning Rate:` This hyperparameter specifies how big the steps are when adjusting the neural network's settings during training. Too big, and you might skip over the best setting; too small, and it'll take a very long time to get there.\n",
    "\n",
    "`Backpropagation:` Short for backward propagation of errors. This is like a feedback system that tells each part of the neural network how much it contributed to any mistakes, so it can learn and do better next time.\n",
    "\n",
    "`Loss functions:` They measure how well a model is performing by calculating the difference between the model's predictions and the actual results.\n",
    "\n",
    "`Cross entropy loss:` This is a measure used when a model needs to choose between categories (like whether an image shows a cat or a dog), and it shows how well the model's predictions align with the actual categories.\n",
    "\n",
    "`Mean squared error:` This shows the average of the squares of the differences between predicted numbers (like a predicted price) and the actual numbers. It's often used for predicting continuous values rather than categories.\n",
    "\n",
    "`Gradients:` Directions and amounts by which a function increases most. The parameters can be changed in a direction opposite to the gradient of the loss function in order to reduce the loss.\n",
    "\n",
    "`Learning Rate:` This hyperparameter specifies how big the steps are when adjusting the neural network's settings during training. Too big, and you might skip over the best setting; too small, and it'll take a very long time to get there.\n",
    "\n",
    "`Momentum:` A technique that helps accelerate the optimizer in the right direction and dampens oscillations.\n",
    "\n",
    "`PyTorch Dataset class:` This is like a recipe that tells your computer how to get the data it needs to learn from, including where to find it and how to parse it, if necessary.\n",
    "\n",
    "`PyTorch Data Loader:` Think of this as a delivery truck that brings the data to your AI in small, manageable loads called batches; this makes it easier for the AI to process and learn from the data.\n",
    "\n",
    "`Batches:` Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.\n",
    "\n",
    "`Shuffle:` It means mixing up the data so that it's not in the same order every time, which helps the AI learn better.\n",
    "\n",
    "`Training Loop:` The cycle that a neural network goes through many times to learn from the data by making predictions, checking errors, and improving itself.\n",
    "\n",
    "`Batches:` Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.\n",
    "\n",
    "`Epochs:` A complete pass through the entire training dataset. The more epochs, the more the computer goes over the material to learn.\n",
    "\n",
    "`Loss functions:` They measure how well a model is performing by calculating the difference between the model's predictions and the actual results.\n",
    "\n",
    "`Optimizer:` Part of the neural network's brain that makes decisions on how to change the network to get better at its job.\n",
    "\n",
    "`Hugging Face :` is a company making waves in the technology world with its amazing tools for understanding and using human language in computers. Hugging Face offers everything from tokenizers, which help computers make sense of text, to a huge variety of ready-to-go language models, and even a treasure trove of data suited for language tasks.\n",
    "\n",
    "`Tokenizers:` These work like a translator, converting the words we use into smaller parts and creating a secret code that computers can understand and work with.\n",
    "\n",
    "`Models:` These are like the brain for computers, allowing them to learn and make decisions based on information they've been fed.\n",
    "\n",
    "`Datasets:` Think of datasets as textbooks for computer models. They are collections of information that models study to learn and improve.\n",
    "\n",
    "`Trainers:` Trainers are the coaches for computer models. They help these models get better at their tasks by practicing and providing guidance. HuggingFace Trainers implement the PyTorch training loop for you, so you can focus instead on other aspects of working on the model.\n",
    "\n",
    "`Tokenization:` It's like cutting a sentence into individual pieces, such as words or characters, to make it easier to analyze.\n",
    "\n",
    "`Tokens:` These are the pieces you get after cutting up text during tokenization, kind of like individual Lego blocks that can be words, parts of words, or even single letters. These tokens are converted to numerical values for models to understand.\n",
    "\n",
    "`Pre-trained Model:` This is a ready-made model that has been previously taught with a lot of data.\n",
    "\n",
    "`Uncased:` This means that the model treats uppercase and lowercase letters as the same.\n",
    "\n",
    "`Truncating:` This refers to shortening longer pieces of text to fit a certain size limit.\n",
    "\n",
    "`Padding:` Adding extra data to shorter texts to reach a uniform length for processing.\n",
    "\n",
    "`Batches:` Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.\n",
    "\n",
    "`Batch Size:` The number of data samples that the machine considers in one go during training.\n",
    "\n",
    "`Epochs:` A complete pass through the entire training dataset. The more epochs, the more the computer goes over the material to learn.\n",
    "\n",
    "`Dataset Splits:` Dividing the dataset into parts for different uses, such as training the model and testing how well it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72a5e6-21c8-4d87-a43d-1193640f91ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2b9f7-e96e-4421-b886-794da80777db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
